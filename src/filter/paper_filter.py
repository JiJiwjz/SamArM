"""
è®ºæ–‡ä¸»é¢˜åˆ†ç±»ä¸ç­›é€‰æ¨¡å—
è´Ÿè´£å¯¹è®ºæ–‡è¿›è¡Œå…³é”®è¯åŒ¹é…ã€ç›¸å…³æ€§è¯„åˆ†å’Œåˆ†ç±»
"""

import re
import logging
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
import json

logger = logging.getLogger(__name__)


@dataclass
class FilteredPaper:
    """ç­›é€‰åçš„è®ºæ–‡ - åŒ…å«åˆ†ç±»å’Œç›¸å…³æ€§ä¿¡æ¯"""
    paper_id: str
    title: str
    authors: List[str]
    summary: str
    published: str
    updated: str
    categories: str
    pdf_url: str
    arxiv_url: str
    fetch_time: str
    
    # ç­›é€‰ç›¸å…³å­—æ®µ
    relevance_score: float              # ç›¸å…³æ€§åˆ†æ•° 0-1
    matched_keywords: List[str]         # åŒ¹é…çš„å…³é”®è¯
    topic_category: str                 # è®ºæ–‡ä¸»é¢˜åˆ†ç±»
    classification_details: Dict        # åˆ†ç±»è¯¦æƒ…
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


class PaperClassifier:
    """è®ºæ–‡åˆ†ç±»å™¨ - å¯¹è®ºæ–‡è¿›è¡Œä¸»é¢˜åˆ†ç±»"""
    
    # ä¸»é¢˜åˆ†ç±»é…ç½®
    TOPIC_KEYWORDS = {
        'image_denoising': {
            'keywords': ['denoise', 'denoising', 'noise removal', 'image quality', 'restoration'],
            'weight': 1.0,
            'description': 'å›¾åƒå»å™ª'
        },
        'image_deraining': {
            'keywords': ['derain', 'deraining', 'rain removal', 'raindrop', 'weather'],
            'weight': 1.0,
            'description': 'å›¾åƒå»é›¨'
        },
        'reinforcement_learning': {
            'keywords': ['reinforcement learning', 'RL', 'Q-learning', 'policy gradient', 'reward', 'agent', 'environment'],
            'weight': 1.0,
            'description': 'å¼ºåŒ–å­¦ä¹ '
        },
        'embodied_ai': {
            'keywords': ['embodied', 'embodied AI', 'robot', 'navigation', 'vision-language-action', 'VLA', 'embodied agent'],
            'weight': 1.0,
            'description': 'å…·èº«æ™ºèƒ½'
        },
        'computer_vision': {
            'keywords': ['vision', 'image', 'visual', 'video', 'detection', 'segmentation', 'recognition'],
            'weight': 0.5,
            'description': 'è®¡ç®—æœºè§†è§‰'
        },
        'deep_learning': {
            'keywords': ['deep learning', 'neural network', 'CNN', 'transformer', 'model', 'network'],
            'weight': 0.3,
            'description': 'æ·±åº¦å­¦ä¹ '
        }
    }
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨"""
        logger.info(f"è®ºæ–‡åˆ†ç±»å™¨å·²åˆå§‹åŒ–ï¼ŒåŒ…å«{len(self.TOPIC_KEYWORDS)}ä¸ªä¸»é¢˜")
    
    def classify_paper(self, paper: Dict) -> Tuple[str, Dict, float]:
        """
        å¯¹å•ç¯‡è®ºæ–‡è¿›è¡Œåˆ†ç±»
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
        
        Returns:
            (ä¸»è¦ä¸»é¢˜, åˆ†ç±»è¯¦æƒ…, ç»¼åˆå¾—åˆ†)
        """
        title = paper.get('title', '').lower()
        summary = paper.get('summary', '').lower()
        
        # ç»„åˆæ–‡æœ¬
        full_text = f"{title}. {summary}"
        
        # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„å¾—åˆ†
        topic_scores = {}
        for topic, config in self.TOPIC_KEYWORDS.items():
            score = self._calculate_topic_score(full_text, config)
            topic_scores[topic] = score
        
        # æŸ¥æ‰¾æœ€é«˜åˆ†ä¸»é¢˜
        main_topic = max(topic_scores, key=topic_scores.get)
        max_score = topic_scores[main_topic]
        
        # åˆ†ç±»è¯¦æƒ…
        classification_details = {
            'main_topic': main_topic,
            'topic_scores': topic_scores,
            'max_score': max_score,
            'description': self.TOPIC_KEYWORDS.get(main_topic, {}).get('description', '')
        }
        
        return main_topic, classification_details, max_score
    
    def _calculate_topic_score(self, text: str, config: Dict) -> float:
        """
        è®¡ç®—æ–‡æœ¬å¯¹æŸä¸ªä¸»é¢˜çš„åŒ¹é…å¾—åˆ†
        
        Args:
            text: å¾…åˆ†ææ–‡æœ¬
            config: ä¸»é¢˜é…ç½®
        
        Returns:
            å¾—åˆ† 0-1
        """
        keywords = config.get('keywords', [])
        weight = config.get('weight', 1.0)
        
        if not keywords:
            return 0.0
        
        # ç»Ÿè®¡åŒ¹é…çš„å…³é”®è¯
        matched_count = 0
        for keyword in keywords:
            # ä½¿ç”¨è¯è¾¹ç•ŒåŒ¹é…ï¼Œé¿å…éƒ¨åˆ†åŒ¹é…
            pattern = rf'\b{re.escape(keyword.lower())}\b'
            if re.search(pattern, text):
                matched_count += 1
        
        # è®¡ç®—å¾—åˆ†ï¼š(åŒ¹é…æ•° / æ€»å…³é”®è¯æ•°) * æƒé‡
        score = (matched_count / len(keywords)) * weight
        
        # é™åˆ¶åœ¨0-1ä¹‹é—´
        return min(score, 1.0)
    
    def get_matched_keywords(self, paper: Dict) -> List[str]:
        """
        è·å–è®ºæ–‡åŒ¹é…çš„æ‰€æœ‰å…³é”®è¯
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
        
        Returns:
            åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨
        """
        title = paper.get('title', '').lower()
        summary = paper.get('summary', '').lower()
        full_text = f"{title}. {summary}"
        
        matched = []
        for topic, config in self.TOPIC_KEYWORDS.items():
            for keyword in config.get('keywords', []):
                pattern = rf'\b{re.escape(keyword.lower())}\b'
                if re.search(pattern, full_text):
                    matched.append(keyword)
        
        return list(set(matched))  # å»é‡


class PaperFilter:
    """è®ºæ–‡ç­›é€‰å™¨ - ç»¼åˆå»é‡ã€åˆ†ç±»ã€ç›¸å…³æ€§è¯„åˆ†"""
    
    def __init__(self, min_relevance_score: float = 0.0):
        """
        åˆå§‹åŒ–ç­›é€‰å™¨
        
        Args:
            min_relevance_score: æœ€ä½ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        self.classifier = PaperClassifier()
        self.min_relevance_score = min_relevance_score
        logger.info(f"è®ºæ–‡ç­›é€‰å™¨å·²åˆå§‹åŒ–ï¼Œæœ€ä½ç›¸å…³æ€§åˆ†æ•°: {min_relevance_score}")
    
    def filter_papers(self, papers: List[Dict]) -> Tuple[List[FilteredPaper], List[Dict]]:
        """
        å¯¹è®ºæ–‡åˆ—è¡¨è¿›è¡Œç­›é€‰
        
        Args:
            papers: åŸå§‹è®ºæ–‡åˆ—è¡¨
        
        Returns:
            (ç­›é€‰åçš„è®ºæ–‡åˆ—è¡¨, è¢«è¿‡æ»¤çš„è®ºæ–‡åŠåŸå› )
        """
        filtered_papers = []
        rejected_papers = []
        
        for paper in papers:
            # è¿›è¡Œåˆ†ç±»å’Œç›¸å…³æ€§è¯„åˆ†
            main_topic, details, score = self.classifier.classify_paper(paper)
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€ä½ç›¸å…³æ€§è¦æ±‚
            if score < self.min_relevance_score:
                rejected_papers.append({
                    'paper': paper,
                    'reason': f'ç›¸å…³æ€§åˆ†æ•°è¿‡ä½: {score:.2f} < {self.min_relevance_score}'
                })
                continue
            
            # è·å–åŒ¹é…çš„å…³é”®è¯
            matched_keywords = self.classifier.get_matched_keywords(paper)
            
            # åˆ›å»ºç­›é€‰åçš„è®ºæ–‡å¯¹è±¡
            filtered_paper = FilteredPaper(
                paper_id=paper.get('paper_id', ''),
                title=paper.get('title', ''),
                authors=paper.get('authors', []),
                summary=paper.get('summary', ''),
                published=paper.get('published', ''),
                updated=paper.get('updated', ''),
                categories=paper.get('categories', ''),
                pdf_url=paper.get('pdf_url', ''),
                arxiv_url=paper.get('arxiv_url', ''),
                fetch_time=paper.get('fetch_time', ''),
                
                relevance_score=score,
                matched_keywords=matched_keywords,
                topic_category=main_topic,
                classification_details=details
            )
            
            filtered_papers.append(filtered_paper)
        
        logger.info(f"ç­›é€‰å®Œæˆ: é€šè¿‡{len(filtered_papers)}ç¯‡, è¢«è¿‡æ»¤{len(rejected_papers)}ç¯‡")
        return filtered_papers, rejected_papers
    
    def filter_and_rank(self, papers: List[Dict], sort_by: str = 'relevance_score') ->  Tuple[List[FilteredPaper], List[Dict]]:
        """
        å¯¹è®ºæ–‡è¿›è¡Œç­›é€‰å’Œæ’åº
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            sort_by: æ’åºæ–¹å¼ ('relevance_score'|'published'|'topic_category')
        
        Returns:
            æ’åºåçš„ç­›é€‰è®ºæ–‡åˆ—è¡¨
        """
        filtered_papers, rejected = self.filter_papers(papers)
        
        # æ’åº
        if sort_by == 'relevance_score':
            filtered_papers.sort(key=lambda p: p.relevance_score, reverse=True)
        elif sort_by == 'published':
            filtered_papers.sort(key=lambda p: p.published, reverse=True)
        elif sort_by == 'topic_category':
            filtered_papers.sort(key=lambda p: p.topic_category)
        
        return filtered_papers, rejected
    
    def group_by_topic(self, papers: List[FilteredPaper]) -> Dict[str, List[FilteredPaper]]:
        """
        æŒ‰ä¸»é¢˜å¯¹è®ºæ–‡è¿›è¡Œåˆ†ç»„
        
        Args:
            papers: ç­›é€‰åçš„è®ºæ–‡åˆ—è¡¨
        
        Returns:
            æŒ‰ä¸»é¢˜åˆ†ç»„çš„è®ºæ–‡å­—å…¸
        """
        grouped = {}
        
        for paper in papers:
            topic = paper.topic_category
            if topic not in grouped:
                grouped[topic] = []
            grouped[topic].append(paper)
        
        return grouped
    
    def get_statistics(self, papers: List[FilteredPaper]) -> Dict:
        """
        è·å–ç­›é€‰åè®ºæ–‡çš„ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            papers: ç­›é€‰åçš„è®ºæ–‡åˆ—è¡¨
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not papers:
            return {
                'total': 0,
                'avg_relevance_score': 0.0,
                'topics': {},
                'keywords_frequency': {}
            }
        
        # æŒ‰ä¸»é¢˜ç»Ÿè®¡
        topics = {}
        keywords_freq = {}
        total_score = 0.0
        
        for paper in papers:
            topic = paper.topic_category
            topics[topic] = topics.get(topic, 0) + 1
            
            total_score += paper.relevance_score
            
            for keyword in paper.matched_keywords:
                keywords_freq[keyword] = keywords_freq.get(keyword, 0) + 1
        
        return {
            'total': len(papers),
            'avg_relevance_score': total_score / len(papers),
            'topics': topics,
            'keywords_frequency': keywords_freq
        }


def main():
    """æµ‹è¯•ç­›é€‰åŠŸèƒ½"""
    import sys
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    
    from src.crawler import ArxivCrawler
    from src.config import ConfigManager
    
    # è·å–é…ç½®å’Œè®ºæ–‡
    config_manager = ConfigManager()
    arxiv_config = config_manager.get_arxiv_config()
    
    crawler = ArxivCrawler(arxiv_config)
    papers = crawler.fetch_papers(days_back=3)
    
    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    papers_dict = [p.to_dict() for p in papers]
    
    # åˆ›å»ºç­›é€‰å™¨
    filter_obj = PaperFilter(min_relevance_score=0.0)
    
    # ç­›é€‰è®ºæ–‡
    filtered_papers, rejected = filter_obj.filter_and_rank(papers_dict, sort_by='relevance_score')
    
    print(f"\nâœ… ç­›é€‰å®Œæˆ: {len(filtered_papers)} ç¯‡è®ºæ–‡é€šè¿‡")
    print(f"âŒ è¢«è¿‡æ»¤: {len(rejected)} ç¯‡è®ºæ–‡\n")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = filter_obj.get_statistics(filtered_papers)
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»æ•°: {stats['total']}")
    print(f"  å¹³å‡ç›¸å…³æ€§åˆ†æ•°: {stats['avg_relevance_score']:.2f}")
    print(f"  æŒ‰ä¸»é¢˜åˆ†å¸ƒ: {stats['topics']}")
    print(f"  é«˜é¢‘å…³é”®è¯: {dict(sorted(stats['keywords_frequency'].items(), key=lambda x: x[1], reverse=True)[:10])}\n")
    
    # æ˜¾ç¤ºå‰5ç¯‡é«˜ç›¸å…³æ€§è®ºæ–‡
    print("ğŸ“š ç›¸å…³æ€§æœ€é«˜çš„5ç¯‡è®ºæ–‡:")
    for i, paper in enumerate(filtered_papers[:5], 1):
        print(f"\n{i}. {paper.title}")
        print(f"   ä¸»é¢˜: {paper.topic_category} (ç›¸å…³æ€§: {paper.relevance_score:.2f})")
        print(f"   åŒ¹é…å…³é”®è¯: {', '.join(paper.matched_keywords[:5])}")


if __name__ == '__main__':
    main()
