"""
DailyJob - è®ºæ–‡æ—¥æŠ¥ç¼–æ’ä»»åŠ¡
ä¸²è”ï¼šçˆ¬å– -> å»é‡ -> ç­›é€‰ -> AIæ€»ç»“ -> è´¨é‡è¯„ä¼° -> é‚®ä»¶æ ¼å¼åŒ– -> é‚®ä»¶å‘é€ -> è½ç›˜
"""

import os
import asyncio
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional

from src.config import ConfigManager
from src.crawler import ArxivCrawler
from src.filter import PaperFilter, Deduplicator
from src.extractor import IdeaExtractor, ExtractedIdea
from src.evaluator import PaperEvaluator  # ğŸ†• å¯¼å…¥è´¨é‡è¯„ä¼°å™¨
from src.sender import EmailFormatter, EmailSender

logger = logging.getLogger(__name__)


class DailyJob:
    """è®ºæ–‡æ—¥æŠ¥ç¼–æ’ä»»åŠ¡"""

    def __init__(self, config_manager: Optional[ConfigManager] = None):
        self.cm = config_manager or ConfigManager()
        self.output_dir = os.path.join(os.getcwd(), "out")
        os.makedirs(self.output_dir, exist_ok=True)

    def _merge_meta(self, metas: List[Dict[str, Any]], ideas: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°†ç­›é€‰é˜¶æ®µå…ƒæ•°æ®åˆå¹¶è¿›AIæ€»ç»“ç»“æœï¼Œä¿ç•™ topic_category / relevance_score / matched_keywords ç­‰"""
        meta_map = {m.get("paper_id"): m for m in metas}
        merged = []
        for idea in ideas:
            pid = idea.get("paper_id")
            base = meta_map.get(pid, {})
            merged.append({**base, **idea})
        return merged
    
    def _merge_quality(self, papers: List[Dict[str, Any]], qualities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ğŸ†• å°†è´¨é‡è¯„ä¼°ç»“æœåˆå¹¶åˆ°è®ºæ–‡æ•°æ®ä¸­"""
        quality_map = {q.get("paper_id"): q for q in qualities}
        merged = []
        for paper in papers:
            pid = paper.get("paper_id")
            quality_data = quality_map.get(pid, {})
            # æå–å…³é”®è¯„ä¼°å­—æ®µ
            paper_with_quality = {
                **paper,
                'quality_score': quality_data.get('overall_score'),
                'quality_level': quality_data.get('quality_level'),
                'quality_reasoning': quality_data.get('reasoning'),
                'innovation_score': quality_data.get('innovation_score'),
                'practicality_score': quality_data.get('practicality_score'),
                'technical_depth_score': quality_data.get('technical_depth_score'),
                'experimental_rigor_score': quality_data.get('experimental_rigor_score'),
                'impact_potential_score': quality_data.get('impact_potential_score'),
                'strengths': quality_data.get('strengths', []),
                'weaknesses': quality_data.get('weaknesses', [])
            }
            merged.append(paper_with_quality)
        return merged

    async def _extract_async(self, filtered_dict: List[Dict[str, Any]], batch_size: int) -> List[Dict[str, Any]]:
        """å†…éƒ¨å¼‚æ­¥AIæ€»ç»“æµç¨‹"""
        deepseek_config = self.cm.get_deepseek_config()
        ideas: List[ExtractedIdea] = []
        try:
            extractor = IdeaExtractor(deepseek_config)
            extracted_ideas, stats = await extractor.extract_batch_papers(filtered_dict, batch_size=batch_size)
            logger.info(f"AIæ€»ç»“å®Œæˆ: æˆåŠŸ{stats['success']} å¤‡é€‰{stats['fallback']} å¤±è´¥{stats['error']} è€—æ—¶{stats['processing_time']:.2f}s")
            ideas = extracted_ideas
        except Exception as e:
            logger.warning(f"AIæ€»ç»“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ‘˜è¦å¤‡é€‰æ–¹æ¡ˆã€‚åŸå› : {e}")
            # é€€åŒ–ä¸ºç›´æ¥æŠŠæ‘˜è¦ä½œä¸º ai_summary
            for p in filtered_dict:
                ideas.append(ExtractedIdea(
                    paper_id=p.get('paper_id', ''),
                    title=p.get('title', ''),
                    authors=p.get('authors', []),
                    summary=p.get('summary', ''),
                    ai_summary=(p.get('summary') or '')[:300] + ('...' if len(p.get('summary','')) > 300 else ''),
                    key_points=None,
                    extraction_status='fallback',
                    extraction_error='DeepSeek unavailable',
                    extraction_time=datetime.utcnow().isoformat(),
                    published=p.get('published', ''),
                    arxiv_url=p.get('arxiv_url', '')
                ))
        return [i.to_dict() for i in ideas]
    
    async def _evaluate_async(self, papers: List[Dict[str, Any]], batch_size: int) -> List[Dict[str, Any]]:
        """ğŸ†• å†…éƒ¨å¼‚æ­¥è´¨é‡è¯„ä¼°æµç¨‹"""
        deepseek_config = self.cm.get_deepseek_config()
        try:
            evaluator = PaperEvaluator(deepseek_config)
            qualities, stats = await evaluator.evaluate_batch_papers(papers, batch_size=batch_size)
            logger.info(f"è´¨é‡è¯„ä¼°å®Œæˆ: æˆåŠŸ{stats['success']} å¤‡é€‰{stats['fallback']} å¤±è´¥{stats['error']} è€—æ—¶{stats['processing_time']:.2f}s")
            return [q.to_dict() for q in qualities]
        except Exception as e:
            logger.warning(f"è´¨é‡è¯„ä¼°ä¸å¯ç”¨: {e}")
            return []

    def run(self,
            days_back: int = 3,
            top_n: int = 10,
            summary_batch_size: int = 3,
            only_new: bool = True,
            send_email: bool = True,
            html_out: Optional[str] = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œä¸€æ¬¡æ—¥æŠ¥ä»»åŠ¡

        Args:
            days_back: å›æº¯å¤©æ•°
            top_n: å‘é€å‰å–TopNç¯‡
            summary_batch_size: AIå¹¶å‘æ‰¹å¤§å°
            only_new: Trueä»…æ¨é€æ–°è®ºæ–‡ï¼ˆå·²å¤„ç†è¿‡çš„ä¸å†æ¨é€ï¼‰
            send_email: æ˜¯å¦å‘é€é‚®ä»¶
            html_out: æŒ‡å®šHTMLè¾“å‡ºè·¯å¾„ï¼Œé»˜è®¤å†™å…¥ out/daily_YYYYMMDD.html

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        start = datetime.utcnow()
        stats: Dict[str, Any] = {
            "start_at": start.isoformat(),
            "days_back": days_back,
            "top_n": top_n,
            "only_new": only_new,
            "send_email": send_email
        }

        # 1) çˆ¬å–
        arxiv_config = self.cm.get_arxiv_config()
        crawler = ArxivCrawler(arxiv_config)
        papers = crawler.fetch_papers(days_back=days_back)
        papers_dict = [p.to_dict() for p in papers]
        stats["fetched"] = len(papers_dict)
        logger.info(f"çˆ¬å–å®Œæˆ: {len(papers_dict)} ç¯‡")

        # 2) å»é‡ï¼ˆæŒä¹…ç¼“å­˜ï¼‰
        dedup = Deduplicator()
        unique_papers, duplicate_papers = dedup.deduplicate_papers(papers_dict)
        if only_new:
            candidate = unique_papers
        else:
            candidate = papers_dict
        stats["unique"] = len(unique_papers)
        stats["duplicates"] = len(duplicate_papers)
        stats["candidates"] = len(candidate)
        logger.info(f"å»é‡å®Œæˆ: æ–°å¢{len(unique_papers)} é‡å¤{len(duplicate_papers)} ç”¨äºç­›é€‰{len(candidate)}")

        # 3) ç­›é€‰ä¸æ’åºï¼ˆæŒ‰ç›¸å…³æ€§ï¼‰
        filter_obj = PaperFilter(min_relevance_score=0.0)
        filtered_papers, _ = filter_obj.filter_and_rank(candidate, sort_by='relevance_score')
        filtered_dict = [p.to_dict() for p in filtered_papers]
        if top_n and len(filtered_dict) > top_n:
            filtered_dict = filtered_dict[:top_n]
        stats["filtered"] = len(filtered_dict)
        logger.info(f"ç­›é€‰å®Œæˆ: é€‰å–{len(filtered_dict)} ç¯‡ç”¨äºAIæ€»ç»“")

        # 4) AI æ€»ç»“ï¼ˆå¼‚æ­¥ï¼‰
        ideas_dict: List[Dict[str, Any]] = asyncio.run(self._extract_async(filtered_dict, batch_size=summary_batch_size))
        stats["summarized"] = len(ideas_dict)

        # 5) åˆå¹¶å…ƒæ•°æ®ï¼Œç¡®ä¿ä¸»é¢˜/ç›¸å…³æ€§åœ¨é‚®ä»¶ä¸­æ˜¾ç¤º
        merged_papers = self._merge_meta(filtered_dict, ideas_dict)
        
        # ğŸ†• 6) è´¨é‡è¯„ä¼°ï¼ˆå¼‚æ­¥ï¼Œä½¿ç”¨ç›¸åŒçš„batch_sizeï¼‰
        quality_dict: List[Dict[str, Any]] = asyncio.run(self._evaluate_async(merged_papers, batch_size=summary_batch_size))
        stats["evaluated"] = len(quality_dict)
        
        # ğŸ†• 7) åˆå¹¶è´¨é‡è¯„ä¼°ç»“æœ
        final_papers = self._merge_quality(merged_papers, quality_dict)
        
        # ğŸ†• 8) æŒ‰è´¨é‡è¯„åˆ†é‡æ–°æ’åºï¼ˆè´¨é‡è¯„åˆ†ä¼˜å…ˆï¼Œç›¸å…³æ€§æ¬¡ä¹‹ï¼‰
        final_papers = sorted(
            final_papers,
            key=lambda p: (
                p.get('quality_score', 0) * 0.7 +  # è´¨é‡è¯„åˆ†æƒé‡70%
                p.get('relevance_score', 0) * 10 * 0.3  # ç›¸å…³æ€§æƒé‡30%
            ),
            reverse=True
        )

        # 9) æ ¼å¼åŒ–é‚®ä»¶
        formatter = EmailFormatter()
        html, email_stats = formatter.format_papers_to_html(final_papers)
        plain = formatter.generate_plain_text_email(final_papers)
        stats["email_stats"] = email_stats

        # 10) å‘é€é‚®ä»¶ï¼ˆå¯é€‰ï¼‰
        sent_stats = None
        if send_email:
            email_config = self.cm.get_email_config()
            recipients = email_config.get('recipients', [])
            if recipients and email_config.get('sender_email'):
                sender = EmailSender(email_config)
                subject = f"ã€Arxivè®ºæ–‡æ—¥æŠ¥ã€‘{datetime.utcnow().strftime('%Y-%m-%d')}"
                # ğŸ”§ ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å send_batch_emails
                sent_stats = sender.send_batch_emails(recipients, subject, html, plain)
                stats["send_result"] = sent_stats
                logger.info(f"é‚®ä»¶å‘é€å®Œæˆ: {sent_stats}")
            else:
                logger.warning("é‚®ä»¶é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡å‘é€")

        # 11) è½ç›˜
        if not html_out:
            html_out = os.path.join(self.output_dir, f"daily_{datetime.utcnow().strftime('%Y%m%d')}.html")
        with open(html_out, 'w', encoding='utf-8') as f:
            f.write(html)
        logger.info(f"HTMLå·²ä¿å­˜: {html_out}")

        report_path = os.path.join(self.output_dir, f"report_{datetime.utcnow().strftime('%Y%m%d')}.json")
        import json
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        stats["html_out"] = html_out
        stats["report_out"] = report_path
        stats["end_at"] = datetime.utcnow().isoformat()
        
        return stats
