"""
è®ºæ–‡æ ¸å¿ƒæ€æƒ³æå–æ¨¡å—
è´Ÿè´£å¯¹è®ºæ–‡è¿›è¡ŒAIæ€»ç»“å’Œæ€æƒ³æå–
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import json

from .deepseek_client import DeepSeekClient, DeepSeekBatchProcessor

logger = logging.getLogger(__name__)


@dataclass
class ExtractedIdea:
    """æå–çš„è®ºæ–‡æ€æƒ³"""
    paper_id: str
    title: str
    authors: List[str]
    summary: str                    # åŸå§‹æ‘˜è¦
    ai_summary: str                 # AIç”Ÿæˆçš„æ€»ç»“
    key_points: Optional[str]       # å…³é”®è¦ç‚¹
    
    extraction_status: str          # success|fallback|error
    extraction_error: Optional[str] # é”™è¯¯ä¿¡æ¯
    extraction_time: str            # æå–æ—¶é—´
    
    # åŸè®ºæ–‡ä¿¡æ¯
    published: str
    arxiv_url: str
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


class IdeaExtractor:
    """è®ºæ–‡æ€æƒ³æå–å™¨"""
    
    def __init__(self, deepseek_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–æ€æƒ³æå–å™¨
        
        Args:
            deepseek_config: DeepSeeké…ç½®å­—å…¸
        """
        api_key = deepseek_config.get('api_key')
        if not api_key:
            raise ValueError("DeepSeek APIå¯†é’¥æœªé…ç½®ï¼")
        
        self.client = DeepSeekClient(
            api_key=api_key,
            api_url=deepseek_config.get('api_url', 'https://api.deepseek.com/v1'),
            model=deepseek_config.get('model', 'deepseek-chat'),
            timeout=deepseek_config.get('timeout', 30)
        )
        
        self.system_prompt = deepseek_config.get('system_prompt', 
            """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯è®ºæ–‡æ€»ç»“ä¸“å®¶ã€‚è¯·ç”¨ä¸­æ–‡æ€»ç»“ä»¥ä¸‹è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³ï¼Œ
åŒ…æ‹¬ï¼š1) ç ”ç©¶é—®é¢˜ 2) æ–¹æ³•åˆ›æ–° 3) ä¸»è¦è´¡çŒ® 4) å®éªŒç»“æœã€‚
æ§åˆ¶åœ¨200-300å­—ä»¥å†…ï¼Œè¯­è¨€ç®€æ´å­¦æœ¯ã€‚""")
        
        logger.info("è®ºæ–‡æ€æƒ³æå–å™¨å·²åˆå§‹åŒ–")
    
    async def extract_single_paper(self, paper: Dict[str, Any]) -> ExtractedIdea:
        """
        å¼‚æ­¥æå–å•ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³
        
        Args:
            paper: è®ºæ–‡ä¿¡æ¯å­—å…¸
        
        Returns:
            æå–çš„æ€æƒ³å¯¹è±¡
        """
        paper_id = paper.get('paper_id', 'unknown')
        
        try:
            # è°ƒç”¨APIæ€»ç»“è®ºæ–‡
            ai_summary = await self.client.summarize_paper(
                paper.get('title', ''),
                paper.get('summary', ''),
                self.system_prompt
            )
            
            if ai_summary:
                extraction_status = 'success'
                extraction_error = None
                logger.info(f"âœ… æ€æƒ³æå–æˆåŠŸ: {paper_id}")
            else:
                # APIå¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ
                ai_summary = self._fallback_summary(paper.get('summary', ''))
                extraction_status = 'fallback'
                extraction_error = 'APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ'
                logger.warning(f"âš ï¸ ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ: {paper_id}")
            
            # åˆ›å»ºæå–ç»“æœ
            idea = ExtractedIdea(
                paper_id=paper_id,
                title=paper.get('title', ''),
                authors=paper.get('authors', []),
                summary=paper.get('summary', ''),
                ai_summary=ai_summary,
                key_points=None,  # å¯é€‰ï¼Œæš‚ä¸æå–
                extraction_status=extraction_status,
                extraction_error=extraction_error,
                extraction_time=datetime.utcnow().isoformat(),
                published=paper.get('published', ''),
                arxiv_url=paper.get('arxiv_url', '')
            )
            
            return idea
        
        except Exception as e:
            logger.error(f"âŒ æ€æƒ³æå–å¤±è´¥: {paper_id}: {e}")
            
            idea = ExtractedIdea(
                paper_id=paper_id,
                title=paper.get('title', ''),
                authors=paper.get('authors', []),
                summary=paper.get('summary', ''),
                ai_summary=self._fallback_summary(paper.get('summary', '')),
                key_points=None,
                extraction_status='error',
                extraction_error=str(e),
                extraction_time=datetime.utcnow().isoformat(),
                published=paper.get('published', ''),
                arxiv_url=paper.get('arxiv_url', '')
            )
            
            return idea
    
    @staticmethod
    def _fallback_summary(summary: str, max_length: int = 300) -> str:
        """
        å¤‡é€‰æ–¹æ¡ˆï¼šç”Ÿæˆè®ºæ–‡æ‘˜è¦çš„è‡ªåŠ¨ç¼©å†™
        
        Args:
            summary: è®ºæ–‡æ‘˜è¦
            max_length: æœ€å¤§é•¿åº¦
        
        Returns:
            ç¼©å†™åçš„æ‘˜è¦
        """
        if not summary:
            return "è®ºæ–‡æ‘˜è¦ç¼ºå¤±"
        
        # æŒ‰å¥å·åˆ†å‰²
        sentences = summary.split('ã€‚')
        result = ''
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(result) + len(sentence) + 1 <= max_length:
                result += sentence + 'ã€‚'
            else:
                break
        
        if not result:
            # å¦‚æœæ²¡æœ‰å¥å·ï¼Œç›´æ¥æˆªæ–­
            result = summary[:max_length]
            if len(summary) > max_length:
                result += '...'
        
        return result.strip()
    
    async def extract_batch_papers(self, papers: List[Dict[str, Any]], 
                                   batch_size: int = 5) -> Tuple[List[ExtractedIdea], Dict[str, Any]]:
        """
        å¼‚æ­¥æ‰¹é‡æå–è®ºæ–‡æ€æƒ³
        
        Args:
            papers: è®ºæ–‡åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
        
        Returns:
            (æå–ç»“æœåˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯)
        """
        logger.info(f"å¼€å§‹æ‰¹é‡æå– {len(papers)} ç¯‡è®ºæ–‡çš„æ€æƒ³ï¼ˆæ‰¹å¤§å°: {batch_size}ï¼‰...")
        
        extracted_ideas = []
        stats = {
            'total': len(papers),
            'success': 0,
            'fallback': 0,
            'error': 0,
            'processing_time': 0
        }
        
        start_time = datetime.utcnow()
        
        # åˆ†æ‰¹å¼‚æ­¥å¤„ç†
        for i in range(0, len(papers), batch_size):
            batch = papers[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            logger.info(f"å¤„ç†ç¬¬ {batch_num} æ‰¹ ({len(batch)} ç¯‡)...")
            
            # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
            tasks = [self.extract_single_paper(paper) for paper in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    logger.error(f"å¤„ç†å‡ºç°å¼‚å¸¸: {result}")
                    stats['error'] += 1
                else:
                    extracted_ideas.append(result)
                    if result.extraction_status == 'success':
                        stats['success'] += 1
                    elif result.extraction_status == 'fallback':
                        stats['fallback'] += 1
                    else:
                        stats['error'] += 1
        
        end_time = datetime.utcnow()
        stats['processing_time'] = (end_time - start_time).total_seconds()
        
        logger.info(f"æ‰¹é‡æå–å®Œæˆ: æˆåŠŸ{stats['success']}, å¤‡é€‰{stats['fallback']}, å¤±è´¥{stats['error']}, "
                   f"è€—æ—¶{stats['processing_time']:.2f}ç§’")
        
        return extracted_ideas, stats


def main():
    """æµ‹è¯•æ€æƒ³æå–åŠŸèƒ½"""
    import sys
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    
    from src.crawler import ArxivCrawler
    from src.config import ConfigManager
    from src.filter import PaperFilter
    
    async def run():
        # è·å–é…ç½®
        config_manager = ConfigManager()
        arxiv_config = config_manager.get_arxiv_config()
        deepseek_config = config_manager.get_deepseek_config()
        
        # çˆ¬å–è®ºæ–‡
        crawler = ArxivCrawler(arxiv_config)
        papers = crawler.fetch_papers(days_back=3)
        papers_dict = [p.to_dict() for p in papers]
        
        # ç­›é€‰è®ºæ–‡
        filter_obj = PaperFilter()
        filtered_papers, _ = filter_obj.filter_papers(papers_dict)
        
        # åªå–å‰3ç¯‡è¿›è¡Œæµ‹è¯•
        test_papers = [p.to_dict() for p in filtered_papers[:3]]
        
        print(f"\nğŸ“š å‡†å¤‡æå– {len(test_papers)} ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³...\n")
        
        # åˆ›å»ºæå–å™¨
        extractor = IdeaExtractor(deepseek_config)
        
        # æå–è®ºæ–‡æ€æƒ³
        extracted_ideas, stats = await extractor.extract_batch_papers(test_papers, batch_size=2)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\n{'='*70}")
        print(f"ğŸ“Š æå–ç»Ÿè®¡: æˆåŠŸ{stats['success']}, å¤‡é€‰{stats['fallback']}, å¤±è´¥{stats['error']}")
        print(f"â±ï¸  è€—æ—¶: {stats['processing_time']:.2f}ç§’")
        print(f"{'='*70}\n")
        
        for i, idea in enumerate(extracted_ideas, 1):
            print(f"ã€è®ºæ–‡ {i}ã€‘")
            print(f"æ ‡é¢˜: {idea.title}")
            print(f"ä½œè€…: {', '.join(idea.authors[:2])}...")
            print(f"çŠ¶æ€: {idea.extraction_status}")
            print(f"\nğŸ¤– AIæ€»ç»“:")
            print(f"{idea.ai_summary}\n")
            print(f"é“¾æ¥: {idea.arxiv_url}\n")
            print("-" * 70 + "\n")
    
    asyncio.run(run())


if __name__ == '__main__':
    main()
