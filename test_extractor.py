#!/usr/bin/env python3
"""
æµ‹è¯•è®ºæ–‡æ€æƒ³æå–æ¨¡å—
å®Œæ•´æµç¨‹ï¼šçˆ¬å– -> ç­›é€‰ -> AIæ€»ç»“æå–
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import asyncio
from datetime import datetime

from src.crawler import ArxivCrawler
from src.config import ConfigManager
from src.filter import PaperFilter, Deduplicator
from src.extractor import IdeaExtractor


async def main():
    """å®Œæ•´æµ‹è¯•æµç¨‹"""
    
    print("\n" + "="*80)
    print("ğŸš€ å®Œæ•´æµç¨‹æµ‹è¯•ï¼šçˆ¬å– -> å»é‡ -> ç­›é€‰ -> AIæ€»ç»“")
    print("="*80 + "\n")
    
    # ===== ç¬¬1æ­¥ï¼šçˆ¬å–è®ºæ–‡ =====
    print("ğŸ“¥ ç¬¬1æ­¥ï¼šä»arxivçˆ¬å–è®ºæ–‡...")
    print("-" * 80)
    
    config_manager = ConfigManager()
    arxiv_config = config_manager.get_arxiv_config()
    
    crawler = ArxivCrawler(arxiv_config)
    papers = crawler.fetch_papers(days_back=3)
    papers_dict = [p.to_dict() for p in papers]
    
    print(f"âœ… æˆåŠŸçˆ¬å– {len(papers_dict)} ç¯‡è®ºæ–‡\n")
    
    # ===== ç¬¬2æ­¥ï¼šå»é‡ =====
    print("ğŸ”„ ç¬¬2æ­¥ï¼šè®ºæ–‡å»é‡...")
    print("-" * 80)
    
    deduplicator = Deduplicator()
    unique_papers, duplicate_papers = deduplicator.deduplicate_papers(papers_dict)
    
    print(f"âœ… å»é‡å®Œæˆ: æ–°å¢{len(unique_papers)}, é‡å¤{len(duplicate_papers)}\n")
    
    # ===== ç¬¬3æ­¥ï¼šç­›é€‰å’Œåˆ†ç±» =====
    print("ğŸ·ï¸  ç¬¬3æ­¥ï¼šè®ºæ–‡ç­›é€‰ä¸åˆ†ç±»...")
    print("-" * 80)
    
    filter_obj = PaperFilter(min_relevance_score=0.0)
    filtered_papers, rejected = filter_obj.filter_and_rank(unique_papers, sort_by='relevance_score')
    
    print(f"âœ… ç­›é€‰å®Œæˆ: é€šè¿‡{len(filtered_papers)}, è¢«è¿‡æ»¤{len(rejected)}\n")
    
    # å°†AIæ€»ç»“æ•°é‡ä»5ç¯‡æ”¹ä¸º10ç¯‡
    test_papers = [p.to_dict() for p in filtered_papers[:10]]
    
    # ===== ç¬¬4æ­¥ï¼šAIæ€»ç»“æå– =====
    print("ğŸ¤– ç¬¬4æ­¥ï¼šAIæ ¸å¿ƒæ€æƒ³æå–...")
    print("-" * 80)
    
    deepseek_config = config_manager.get_deepseek_config()
    
    if not deepseek_config.get('api_key'):
        print("âŒ DeepSeek APIå¯†é’¥æœªé…ç½®ï¼")
        print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® DEEPSEEK_API_KEY")
        return
    
    print(f"ğŸ“‹ å‡†å¤‡æå– {len(test_papers)} ç¯‡è®ºæ–‡çš„æ ¸å¿ƒæ€æƒ³...\n")
    
    extractor = IdeaExtractor(deepseek_config)
    extracted_ideas, stats = await extractor.extract_batch_papers(test_papers, batch_size=2)
    
    # ===== ç»“æœå±•ç¤º =====
    print("\n" + "="*80)
    print("ğŸ“Š AIæ€»ç»“æå–ç»“æœ")
    print("="*80)
    print(f"æ€»æ•°: {stats['total']}")
    print(f"æˆåŠŸ: {stats['success']}")
    print(f"å¤‡é€‰: {stats['fallback']}")
    print(f"å¤±è´¥: {stats['error']}")
    print(f"è€—æ—¶: {stats['processing_time']:.2f}ç§’")
    print("="*80 + "\n")
    
    # è¯¦ç»†æ˜¾ç¤ºæå–ç»“æœ
    for i, idea in enumerate(extracted_ideas, 1):
        print(f"ã€è®ºæ–‡ {i}ã€‘")
        print(f"è®ºæ–‡ID: {idea.paper_id}")
        print(f"æ ‡é¢˜: {idea.title}")
        print(f"ä½œè€…: {', '.join(idea.authors[:3])}...")
        print(f"å‘å¸ƒ: {idea.published[:10]}")
        print(f"çŠ¶æ€: {idea.extraction_status}")
        
        if idea.extraction_error:
            print(f"é”™è¯¯: {idea.extraction_error}")
        
        print(f"\nğŸ“ åŸå§‹æ‘˜è¦ (å‰200å­—):")
        print(f"{idea.summary[:200]}...\n")
        
        print(f"ğŸ¤– AIç”Ÿæˆæ€»ç»“:")
        print(f"{idea.ai_summary}\n")
        
        print(f"ğŸ”— é“¾æ¥: {idea.arxiv_url}\n")
        print("-" * 80 + "\n")
    
    print("âœ… æµ‹è¯•å®Œæˆï¼\n")


if __name__ == '__main__':
    asyncio.run(main())
