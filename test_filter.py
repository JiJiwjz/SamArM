#!/usr/bin/env python3
"""
æµ‹è¯•è¿‡æ»¤å’Œåˆ†ç±»æ¨¡å—
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.crawler import ArxivCrawler
from src.config import ConfigManager
from src.filter import PaperFilter, Deduplicator

def main():
    """æµ‹è¯•å®Œæ•´æµç¨‹ï¼šçˆ¬å– -> å»é‡ -> åˆ†ç±»ç­›é€‰"""
    
    print("\n" + "="*70)
    print("ğŸš€ è®ºæ–‡çˆ¬å– -> å»é‡ -> åˆ†ç±»ç­›é€‰ å®Œæ•´æµ‹è¯•")
    print("="*70 + "\n")
    
    # ===== ç¬¬1æ­¥ï¼šçˆ¬å–è®ºæ–‡ =====
    print("ğŸ“¥ ç¬¬1æ­¥ï¼šä»arxivçˆ¬å–è®ºæ–‡...")
    print("-" * 70)
    
    config_manager = ConfigManager()
    arxiv_config = config_manager.get_arxiv_config()
    
    crawler = ArxivCrawler(arxiv_config)
    papers = crawler.fetch_papers(days_back=3)
    papers_dict = [p.to_dict() for p in papers]
    
    print(f"âœ… æˆåŠŸçˆ¬å– {len(papers_dict)} ç¯‡è®ºæ–‡\n")
    
    # ===== ç¬¬2æ­¥ï¼šå»é‡ =====
    print("ğŸ”„ ç¬¬2æ­¥ï¼šè®ºæ–‡å»é‡...")
    print("-" * 70)
    
    deduplicator = Deduplicator()
    unique_papers, duplicate_papers = deduplicator.deduplicate_papers(papers_dict)
    
    print(f"âœ… å»é‡å®Œæˆ:")
    print(f"   æ–°å¢è®ºæ–‡: {len(unique_papers)}")
    print(f"   é‡å¤è®ºæ–‡: {len(duplicate_papers)}\n")
    
    if duplicate_papers:
        print("   é‡å¤è®ºæ–‡åˆ—è¡¨:")
        for paper in duplicate_papers[:3]:
            print(f"   - {paper['title'][:60]}... (é‡å¤äº: {paper['duplicate_of']})")
        if len(duplicate_papers) > 3:
            print(f"   ... è¿˜æœ‰ {len(duplicate_papers) - 3} ç¯‡\n")
    
    # ===== ç¬¬3æ­¥ï¼šåˆ†ç±»å’Œç­›é€‰ =====
    print("ğŸ·ï¸  ç¬¬3æ­¥ï¼šè®ºæ–‡åˆ†ç±»ä¸ç›¸å…³æ€§è¯„åˆ†...")
    print("-" * 70)
    
    filter_obj = PaperFilter(min_relevance_score=0.0)
    filtered_papers, rejected = filter_obj.filter_and_rank(unique_papers, sort_by='relevance_score')
    
    print(f"âœ… ç­›é€‰å®Œæˆ:")
    print(f"   é€šè¿‡ç­›é€‰: {len(filtered_papers)}")
    print(f"   è¢«è¿‡æ»¤: {len(rejected)}\n")
    
    # ===== ç¬¬4æ­¥ï¼šç»Ÿè®¡åˆ†æ =====
    print("ğŸ“Š ç¬¬4æ­¥ï¼šç»Ÿè®¡åˆ†æ...")
    print("-" * 70)
    
    stats = filter_obj.get_statistics(filtered_papers)
    
    print(f"æ€»è®ºæ–‡æ•°: {stats['total']}")
    print(f"å¹³å‡ç›¸å…³æ€§åˆ†æ•°: {stats['avg_relevance_score']:.3f}")
    print(f"\næŒ‰ä¸»é¢˜åˆ†å¸ƒ:")
    for topic, count in sorted(stats['topics'].items(), key=lambda x: x[1], reverse=True):
        print(f"  {topic}: {count}")
    
    print(f"\né«˜é¢‘å…³é”®è¯ Top 15:")
    keywords_sorted = sorted(stats['keywords_frequency'].items(), key=lambda x: x[1], reverse=True)
    for keyword, freq in keywords_sorted[:15]:
        print(f"  {keyword}: {freq}")
    
    # ===== ç¬¬5æ­¥ï¼šè¯¦ç»†å±•ç¤º =====
    print("\n" + "="*70)
    print("ğŸ“š ç›¸å…³æ€§æœ€é«˜çš„10ç¯‡è®ºæ–‡")
    print("="*70 + "\n")
    
    for i, paper in enumerate(filtered_papers[:10], 1):
        print(f"ã€è®ºæ–‡ {i}ã€‘")
        print(f"æ ‡é¢˜: {paper.title}")
        print(f"ä½œè€…: {', '.join(paper.authors[:3])}...")
        print(f"å‘å¸ƒæ—¶é—´: {paper.published[:10]}")
        print(f"ä¸»é¢˜åˆ†ç±»: {paper.topic_category} (ç›¸å…³æ€§: {paper.relevance_score:.3f})")
        print(f"åŒ¹é…å…³é”®è¯: {', '.join(paper.matched_keywords[:5])}")
        if len(paper.matched_keywords) > 5:
            print(f"            ... è¿˜æœ‰ {len(paper.matched_keywords) - 5} ä¸ªå…³é”®è¯")
        print(f"é“¾æ¥: {paper.arxiv_url}\n")
    
    # ===== æŒ‰ä¸»é¢˜åˆ†ç»„å±•ç¤º =====
    print("="*70)
    print("ğŸ—‚ï¸  æŒ‰ä¸»é¢˜åˆ†ç»„å±•ç¤º")
    print("="*70 + "\n")
    
    grouped = filter_obj.group_by_topic(filtered_papers)
    
    for topic in sorted(grouped.keys()):
        papers_in_topic = grouped[topic]
        print(f"ã€{topic}ã€‘({len(papers_in_topic)}ç¯‡)")
        for paper in papers_in_topic[:3]:
            print(f"  - {paper.title[:70]}")
        if len(papers_in_topic) > 3:
            print(f"  ... è¿˜æœ‰ {len(papers_in_topic) - 3} ç¯‡")
        print()
    
    print("="*70)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("="*70 + "\n")


if __name__ == '__main__':
    main()
