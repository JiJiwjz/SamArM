#!/usr/bin/env python3
"""
æµ‹è¯•çˆ¬è™«è„šæœ¬
ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œæ­¤è„šæœ¬
"""

import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.crawler import ArxivCrawler
from src.config import ConfigManager

def main():
    """æµ‹è¯•çˆ¬è™«"""
    config_manager = ConfigManager()
    arxiv_config = config_manager.get_arxiv_config()
    
    print(f"\nğŸ“„ arxivçˆ¬è™«é…ç½®:")
    print(f"  å…³é”®è¯: {arxiv_config.get('keywords')}")
    print(f"  åˆ†ç±»: {arxiv_config.get('categories')}")
    print(f"  æœ€å¤§ç»“æœæ•°: {arxiv_config.get('max_results')}")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    print("\nğŸš€ å¼€å§‹çˆ¬å–è®ºæ–‡...")
    crawler = ArxivCrawler(arxiv_config)
    
    # è·å–è®ºæ–‡
    papers = crawler.fetch_papers(days_back=7)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nâœ… æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡\n")
    for i, paper in enumerate(papers[:5], 1):
        print(f"ã€è®ºæ–‡ {i}ã€‘")
        print(f"æ ‡é¢˜: {paper.title}")
        print(f"ä½œè€…: {', '.join(paper.authors[:3])}...")
        print(f"å‘å¸ƒæ—¶é—´: {paper.published}")
        print(f"é“¾æ¥: {paper.arxiv_url}")
        print(f"æ‘˜è¦é¢„è§ˆ: {paper.summary[:150]}...\n")


if __name__ == '__main__':
    main()
